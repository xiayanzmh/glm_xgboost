{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Complete ML Pipeline - GLM vs XGBoost\n",
                "\n",
                "This notebook breaks down the complete machine learning pipeline into executable cells.\n",
                "Each major step is separated for easier execution and debugging.\n",
                "\n",
                "**Pipeline Phases:**\n",
                "1. Setup & Imports\n",
                "2. Exploratory Data Analysis (EDA)\n",
                "3. Data Preprocessing & Feature Engineering\n",
                "4. GLM Model Development\n",
                "5. XGBoost Model Development\n",
                "6. Model Comparison & Selection\n",
                "7. Model Interpretability (LIME)\n",
                "8. Final Documentation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.13.11' requires the jupyter and notebook package.\n",
                        "\u001b[1;31mInstall 'jupyter and notebook' into the Python environment. \n",
                        "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
                        "\u001b[1;31mor\n",
                        "\u001b[1;31mconda install jupyter notebook -U'\n",
                        "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
                    ]
                }
            ],
            "source": [
                "# Import libraries\n",
                "import os\n",
                "import pickle\n",
                "import warnings\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# ML imports\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score, \n",
                "    roc_auc_score, classification_report, confusion_matrix\n",
                ")\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "\n",
                "# XGBoost\n",
                "import xgboost as xgb\n",
                "\n",
                "# Imbalance handling\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "# Interpretability\n",
                "try:\n",
                "    import lime\n",
                "    import lime.lime_tabular\n",
                "    LIME_AVAILABLE = True\n",
                "except ImportError:\n",
                "    LIME_AVAILABLE = False\n",
                "    print(\"LIME not available. Install with: pip install lime\")\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "%matplotlib inline\n",
                "\n",
                "print(\"‚úì All libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup output directory\n",
                "today = datetime.now().strftime('%Y-%m-%d')\n",
                "output_dir = Path(f'../outputs/{today}')\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Create subdirectories\n",
                "for subdir in ['eda', 'preprocessing', 'models', 'results', 'plots']:\n",
                "    (output_dir / subdir).mkdir(exist_ok=True)\n",
                "\n",
                "# Create symlink to latest run\n",
                "latest_link = Path('../outputs/latest')\n",
                "if latest_link.exists() or latest_link.is_symlink():\n",
                "    latest_link.unlink()\n",
                "try:\n",
                "    latest_link.symlink_to(today, target_is_directory=True)\n",
                "except (OSError, NotImplementedError):\n",
                "    pass\n",
                "\n",
                "print(f\"‚úì Output directory created: {output_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "print(\"Loading data...\")\n",
                "df_original = pd.read_excel('../data/Data.xlsx')\n",
                "\n",
                "print(f\"‚úì Data loaded successfully\")\n",
                "print(f\"  Shape: {df_original.shape}\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "df_original.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic information\n",
                "print(\"=\" * 80)\n",
                "print(\"DATASET SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"Total records: {len(df_original):,}\")\n",
                "print(f\"Features: {len([col for col in df_original.columns if col.startswith('V')])} features\")\n",
                "print(f\"Target: Y (binary classification)\")\n",
                "print(f\"Missing values: {df_original.isnull().sum().sum()}\")\n",
                "\n",
                "print(f\"\\nData types:\")\n",
                "df_original.dtypes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target variable analysis\n",
                "print(\"TARGET VARIABLE ANALYSIS\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "target_counts = df_original['Y'].value_counts().sort_index()\n",
                "target_props = df_original['Y'].value_counts(normalize=True).sort_index()\n",
                "\n",
                "print(f\"Class distribution:\")\n",
                "print(target_counts)\n",
                "print(f\"\\nClass proportions:\")\n",
                "print(target_props)\n",
                "\n",
                "# Visualize target distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Count plot\n",
                "target_counts.plot(kind='bar', ax=axes[0], color=['skyblue', 'coral'])\n",
                "axes[0].set_title('Target Variable Distribution (Counts)')\n",
                "axes[0].set_xlabel('Class')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
                "\n",
                "# Proportion plot\n",
                "target_props.plot(kind='bar', ax=axes[1], color=['skyblue', 'coral'])\n",
                "axes[1].set_title('Target Variable Distribution (Proportions)')\n",
                "axes[1].set_xlabel('Class')\n",
                "axes[1].set_ylabel('Proportion')\n",
                "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(output_dir / 'plots' / 'target_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature analysis\n",
                "numeric_features = []\n",
                "categorical_features = []\n",
                "\n",
                "print(\"FEATURE ANALYSIS\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "for col in df_original.columns:\n",
                "    if col.startswith('V'):\n",
                "        if df_original[col].dtype in ['object', 'str', str]:\n",
                "            categorical_features.append(col)\n",
                "            print(f\"{col} (categorical): {df_original[col].nunique()} unique values\")\n",
                "        else:\n",
                "            numeric_features.append(col)\n",
                "\n",
                "print(f\"\\nFeature Summary:\")\n",
                "print(f\"  Numeric features: {len(numeric_features)}\")\n",
                "print(f\"  Categorical features: {len(categorical_features)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation analysis\n",
                "if numeric_features:\n",
                "    print(\"CORRELATION ANALYSIS\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    correlations = df_original[numeric_features + ['Y']].corr()['Y'].sort_values(key=abs, ascending=False)[1:]\n",
                "    \n",
                "    print(\"Top 10 correlations with target:\")\n",
                "    print(correlations.head(10))\n",
                "    \n",
                "    # Visualize top correlations\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    top_corr = correlations.head(15)\n",
                "    colors = ['coral' if x < 0 else 'skyblue' for x in top_corr.values]\n",
                "    top_corr.plot(kind='barh', color=colors)\n",
                "    plt.title('Top 15 Feature Correlations with Target')\n",
                "    plt.xlabel('Correlation Coefficient')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(output_dir / 'plots' / 'feature_correlations.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save EDA summary\n",
                "eda_summary_path = output_dir / 'eda' / 'eda_summary.txt'\n",
                "with open(eda_summary_path, 'w') as f:\n",
                "    f.write(\"=\" * 80 + \"\\n\")\n",
                "    f.write(\"EDA Summary\\n\")\n",
                "    f.write(\"=\" * 80 + \"\\n\")\n",
                "    f.write(f\"Dataset shape: {df_original.shape}\\n\")\n",
                "    f.write(f\"Missing values: {df_original.isnull().sum().sum()}\\n\")\n",
                "    f.write(f\"Target distribution:\\n{target_counts.to_string()}\\n\")\n",
                "    f.write(f\"Numeric features: {len(numeric_features)}\\n\")\n",
                "    f.write(f\"Categorical features: {len(categorical_features)}\\n\")\n",
                "    if numeric_features:\n",
                "        f.write(f\"\\nTop correlations:\\n{correlations.head(10).to_string()}\\n\")\n",
                "\n",
                "print(f\"‚úì EDA summary saved to: {eda_summary_path}\")\n",
                "print(\"‚úì Phase 1 (EDA) completed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Preprocessing & Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copy data for processing\n",
                "print(\"=\" * 80)\n",
                "print(\"DATA PREPROCESSING & FEATURE ENGINEERING\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "df = df_original.copy()\n",
                "print(f\"‚úì Working with copy of original data: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Categorical encoding\n",
                "print(\"\\nCategorical Feature Encoding:\")\n",
                "categorical_features = [col for col in df.columns if col.startswith('V') and df[col].dtype == 'object']\n",
                "\n",
                "label_encoder = LabelEncoder()\n",
                "for col in categorical_features:\n",
                "    print(f\"  Encoding {col}...\")\n",
                "    df[col] = label_encoder.fit_transform(df[col])\n",
                "\n",
                "print(f\"‚úì Encoded {len(categorical_features)} categorical features\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Engineering\n",
                "print(\"\\nFeature Engineering:\")\n",
                "feature_cols = [col for col in df.columns if col.startswith('V')]\n",
                "\n",
                "# Interaction features\n",
                "if 'V13' in feature_cols and 'V3' in feature_cols:\n",
                "    df['V13_V3_interaction'] = df['V13'] * df['V3']\n",
                "    print(\"  ‚úì Created V13_V3_interaction\")\n",
                "\n",
                "if 'V13' in feature_cols and 'V7' in feature_cols:\n",
                "    df['V13_V7_interaction'] = df['V13'] * df['V7']\n",
                "    print(\"  ‚úì Created V13_V7_interaction\")\n",
                "\n",
                "# Polynomial features\n",
                "if 'V13' in feature_cols:\n",
                "    df['V13_squared'] = df['V13'] ** 2\n",
                "    print(\"  ‚úì Created V13_squared\")\n",
                "\n",
                "if 'V3' in feature_cols:\n",
                "    df['V3_squared'] = df['V3'] ** 2\n",
                "    print(\"  ‚úì Created V3_squared\")\n",
                "\n",
                "# Ratio features\n",
                "if 'V2' in feature_cols and 'V18' in feature_cols:\n",
                "    df['V2_V18_ratio'] = df['V2'] / (df['V18'] + 1)\n",
                "    print(\"  ‚úì Created V2_V18_ratio\")\n",
                "\n",
                "# Aggregate features\n",
                "df['feature_sum'] = df[feature_cols].sum(axis=1)\n",
                "df['feature_mean'] = df[feature_cols].mean(axis=1)\n",
                "df['feature_std'] = df[feature_cols].std(axis=1)\n",
                "print(\"  ‚úì Created aggregate features: sum, mean, std\")\n",
                "\n",
                "df_engineered = df\n",
                "print(f\"\\n‚úì Feature engineering completed\")\n",
                "print(f\"  Total features: {len([col for col in df.columns if col not in ['ID', 'Y']])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features and target\n",
                "X = df.drop(['ID', 'Y'], axis=1, errors='ignore')\n",
                "y = df['Y']\n",
                "\n",
                "print(f\"Features: {X.shape[1]} columns\")\n",
                "print(f\"Target distribution: {y.value_counts().sort_index().to_dict()}\")\n",
                "print(f\"\\nFeature columns: {list(X.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create data splits\n",
                "print(\"\\nCreating Data Splits:\")\n",
                "print(\"  Strategy: 65% train / 15% validation / 20% test\\n\")\n",
                "\n",
                "# First split: train+val vs test (80/20)\n",
                "X_temp, X_test, y_temp, y_test = train_test_split(\n",
                "    X, y, test_size=0.20, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Second split: train vs val (65/15 of total)\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_temp, y_temp, test_size=0.1875, random_state=42, stratify=y_temp\n",
                ")\n",
                "\n",
                "print(f\"  Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
                "print(f\"  Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(df)*100:.1f}%)\")\n",
                "print(f\"  Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
                "\n",
                "print(f\"\\n‚úì Data splits created successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Scaling\n",
                "print(\"\\nFeature Scaling:\")\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Convert back to DataFrames\n",
                "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
                "X_val = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
                "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
                "\n",
                "print(f\"  ‚úì Features scaled: {X_train.shape[1]} columns\")\n",
                "print(f\"  Scaling method: StandardScaler (mean=0, std=1)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle class imbalance with SMOTE\n",
                "print(\"\\nHandling Class Imbalance with SMOTE:\")\n",
                "print(f\"  Original class distribution: {y_train.value_counts().sort_index().to_dict()}\")\n",
                "\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
                "\n",
                "# Store SMOTE data\n",
                "X_train_smote = pd.DataFrame(X_train_smote, columns=X_train.columns)\n",
                "y_train_smote = pd.Series(y_train_smote, name=y_train.name)\n",
                "\n",
                "print(f\"  Resampled class distribution: {y_train_smote.value_counts().sort_index().to_dict()}\")\n",
                "print(f\"  Resampled size: {len(X_train_smote):,} samples\")\n",
                "print(f\"\\n‚úì SMOTE resampling completed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate class weights\n",
                "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
                "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
                "\n",
                "print(f\"Calculated class weights: {class_weight_dict}\")\n",
                "print(\"  (For models that support class_weight parameter)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save processed data\n",
                "preprocessing_dir = output_dir / 'preprocessing'\n",
                "\n",
                "# Save datasets\n",
                "X_train.to_pickle(preprocessing_dir / 'X_train.pkl')\n",
                "X_val.to_pickle(preprocessing_dir / 'X_val.pkl')\n",
                "X_test.to_pickle(preprocessing_dir / 'X_test.pkl')\n",
                "y_train.to_pickle(preprocessing_dir / 'y_train.pkl')\n",
                "y_val.to_pickle(preprocessing_dir / 'y_val.pkl')\n",
                "y_test.to_pickle(preprocessing_dir / 'y_test.pkl')\n",
                "X_train_smote.to_pickle(preprocessing_dir / 'X_train_smote.pkl')\n",
                "y_train_smote.to_pickle(preprocessing_dir / 'y_train_smote.pkl')\n",
                "\n",
                "# Save preprocessors\n",
                "with open(preprocessing_dir / 'scaler.pkl', 'wb') as f:\n",
                "    pickle.dump(scaler, f)\n",
                "with open(preprocessing_dir / 'label_encoder.pkl', 'wb') as f:\n",
                "    pickle.dump(label_encoder, f)\n",
                "with open(preprocessing_dir / 'class_weights.pkl', 'wb') as f:\n",
                "    pickle.dump(class_weight_dict, f)\n",
                "\n",
                "print(f\"‚úì All processed data saved to: {preprocessing_dir}\")\n",
                "print(\"‚úì Phase 2 (Preprocessing) completed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. GLM Model Development"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"GLM MODEL DEVELOPMENT\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Use SMOTE data for GLM training\n",
                "X_train_use = X_train_smote\n",
                "y_train_use = y_train_smote\n",
                "\n",
                "print(f\"Training data: {X_train_use.shape[0]:,} samples with {X_train_use.shape[1]} features\")\n",
                "print(f\"Class distribution: {y_train_use.value_counts().sort_index().to_dict()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GLM hyperparameter tuning\n",
                "print(\"\\nGLM Hyperparameter Tuning with GridSearchCV:\")\n",
                "\n",
                "# Parameter grid\n",
                "param_grid = {\n",
                "    'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
                "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
                "    'solver': ['liblinear', 'saga'],\n",
                "    'class_weight': [None, 'balanced']\n",
                "}\n",
                "\n",
                "print(f\"Parameter grid: {param_grid}\")\n",
                "\n",
                "# Create GLM model\n",
                "glm = LogisticRegression(random_state=42, max_iter=1000)\n",
                "\n",
                "# Grid search\n",
                "print(\"\\nRunning GridSearchCV (this may take a while)...\")\n",
                "grid_search = GridSearchCV(\n",
                "    glm, param_grid, cv=5, scoring='roc_auc',\n",
                "    n_jobs=-1, verbose=1\n",
                ")\n",
                "\n",
                "grid_search.fit(X_train_use, y_train_use)\n",
                "\n",
                "glm_model = grid_search.best_estimator_\n",
                "\n",
                "print(f\"\\n‚úì Best GLM parameters: {grid_search.best_params_}\")\n",
                "print(f\"‚úì Best GLM CV score (AUC): {grid_search.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate GLM on all splits\n",
                "def evaluate_model(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test):\n",
                "    \"\"\"Evaluate model on train, validation, and test sets\"\"\"\n",
                "    results = {'model_name': model_name}\n",
                "    \n",
                "    for split_name, X_data, y_data in [\n",
                "        ('train', X_train, y_train),\n",
                "        ('validation', X_val, y_val),\n",
                "        ('test', X_test, y_test)\n",
                "    ]:\n",
                "        # Make predictions\n",
                "        y_pred = model.predict(X_data)\n",
                "        y_pred_proba = model.predict_proba(X_data)[:, 1]\n",
                "        \n",
                "        # Calculate metrics\n",
                "        results[f'{split_name}_accuracy'] = accuracy_score(y_data, y_pred)\n",
                "        results[f'{split_name}_precision'] = precision_score(y_data, y_pred)\n",
                "        results[f'{split_name}_recall'] = recall_score(y_data, y_pred)\n",
                "        results[f'{split_name}_f1'] = f1_score(y_data, y_pred)\n",
                "        results[f'{split_name}_auc'] = roc_auc_score(y_data, y_pred_proba)\n",
                "        \n",
                "        print(f\"\\n{split_name.capitalize()} Results:\")\n",
                "        print(f\"  Accuracy:  {results[f'{split_name}_accuracy']:.4f}\")\n",
                "        print(f\"  Precision: {results[f'{split_name}_precision']:.4f}\")\n",
                "        print(f\"  Recall:    {results[f'{split_name}_recall']:.4f}\")\n",
                "        print(f\"  F1-Score:  {results[f'{split_name}_f1']:.4f}\")\n",
                "        print(f\"  AUC:       {results[f'{split_name}_auc']:.4f}\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "print(\"GLM Model Evaluation:\")\n",
                "glm_results = evaluate_model(glm_model, 'GLM', X_train, y_train, X_val, y_val, X_test, y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save GLM model\n",
                "models_dir = output_dir / 'models'\n",
                "with open(models_dir / 'glm_model.pkl', 'wb') as f:\n",
                "    pickle.dump(glm_model, f)\n",
                "\n",
                "print(f\"‚úì GLM model saved to: {models_dir / 'glm_model.pkl'}\")\n",
                "print(\"‚úì Phase 3 (GLM) completed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. XGBoost Model Development"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"XGBOOST MODEL DEVELOPMENT\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Use original training data with class weights for XGBoost\n",
                "X_train_use = X_train\n",
                "y_train_use = y_train\n",
                "\n",
                "print(f\"Training data: {X_train_use.shape[0]:,} samples with {X_train_use.shape[1]} features\")\n",
                "print(f\"Class distribution: {y_train_use.value_counts().sort_index().to_dict()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate scale_pos_weight for XGBoost\n",
                "scale_pos_weight = len(y_train_use[y_train_use == 0]) / len(y_train_use[y_train_use == 1])\n",
                "print(f\"\\nCalculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
                "print(\"  (Ratio of negative to positive samples for imbalance handling)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost hyperparameter tuning\n",
                "print(\"\\nXGBoost Hyperparameter Tuning with GridSearchCV:\")\n",
                "\n",
                "# Parameter grid (simplified for faster execution)\n",
                "param_grid = {\n",
                "    'n_estimators': [100, 200],\n",
                "    'max_depth': [3, 6],\n",
                "    'learning_rate': [0.1, 0.2],\n",
                "    'scale_pos_weight': [1, scale_pos_weight]\n",
                "}\n",
                "\n",
                "print(f\"Parameter grid: {param_grid}\")\n",
                "\n",
                "# Create XGBoost model\n",
                "xgb_model = xgb.XGBClassifier(\n",
                "    random_state=42,\n",
                "    eval_metric='logloss',\n",
                "    use_label_encoder=False\n",
                ")\n",
                "\n",
                "# Grid search\n",
                "print(\"\\nRunning GridSearchCV (this may take a while)...\")\n",
                "grid_search = GridSearchCV(\n",
                "    xgb_model, param_grid, cv=3, scoring='roc_auc',\n",
                "    n_jobs=-1, verbose=1\n",
                ")\n",
                "\n",
                "grid_search.fit(X_train_use, y_train_use)\n",
                "\n",
                "xgb_model = grid_search.best_estimator_\n",
                "\n",
                "print(f\"\\n‚úì Best XGBoost parameters: {grid_search.best_params_}\")\n",
                "print(f\"‚úì Best XGBoost CV score (AUC): {grid_search.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate XGBoost on all splits\n",
                "print(\"XGBoost Model Evaluation:\")\n",
                "xgb_results = evaluate_model(xgb_model, 'XGBoost', X_train, y_train, X_val, y_val, X_test, y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "print(\"\\nXGBoost Feature Importance:\")\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X_train.columns,\n",
                "    'importance': xgb_model.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(feature_importance.head(15))\n",
                "\n",
                "# Visualize feature importance\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.barh(range(15), feature_importance['importance'].head(15), color='skyblue')\n",
                "plt.yticks(range(15), feature_importance['feature'].head(15))\n",
                "plt.xlabel('Importance')\n",
                "plt.title('XGBoost - Top 15 Feature Importances')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.savefig(output_dir / 'plots' / 'xgb_feature_importance.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save XGBoost model\n",
                "models_dir = output_dir / 'models'\n",
                "with open(models_dir / 'xgboost_model.pkl', 'wb') as f:\n",
                "    pickle.dump(xgb_model, f)\n",
                "\n",
                "print(f\"‚úì XGBoost model saved to: {models_dir / 'xgboost_model.pkl'}\")\n",
                "print(\"‚úì Phase 4 (XGBoost) completed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Comparison & Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"MODEL COMPARISON & SELECTION\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "results = {'glm': glm_results, 'xgboost': xgb_results}\n",
                "\n",
                "comparison_data = []\n",
                "metrics = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'test_auc']\n",
                "\n",
                "for model_name, res in results.items():\n",
                "    row = {'Model': res['model_name']}\n",
                "    for metric in metrics:\n",
                "        if metric in res:\n",
                "            row[metric.replace('test_', '').upper()] = f\"{res[metric]:.4f}\"\n",
                "    comparison_data.append(row)\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "print(\"\\nModel Performance Comparison (Test Set):\")\n",
                "print(\"=\" * 50)\n",
                "print(comparison_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Determine best model\n",
                "best_model_name = None\n",
                "best_auc = 0\n",
                "\n",
                "for model_name, res in results.items():\n",
                "    if res.get('test_auc', 0) > best_auc:\n",
                "        best_auc = res['test_auc']\n",
                "        best_model_name = model_name\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"BEST MODEL: {best_model_name.upper()}\")\n",
                "print(f\"Test AUC: {best_auc:.4f}\")\n",
                "print(f\"{'='*50}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize model comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Comparison bar chart\n",
                "metrics_short = ['ACCURACY', 'PRECISION', 'RECALL', 'F1', 'AUC']\n",
                "glm_scores = [float(comparison_df.loc[comparison_df['Model'] == 'GLM', m].values[0]) for m in metrics_short]\n",
                "xgb_scores = [float(comparison_df.loc[comparison_df['Model'] == 'XGBoost', m].values[0]) for m in metrics_short]\n",
                "\n",
                "x = np.arange(len(metrics_short))\n",
                "width = 0.35\n",
                "\n",
                "axes[0].bar(x - width/2, glm_scores, width, label='GLM', color='skyblue')\n",
                "axes[0].bar(x + width/2, xgb_scores, width, label='XGBoost', color='coral')\n",
                "axes[0].set_xlabel('Metrics')\n",
                "axes[0].set_ylabel('Score')\n",
                "axes[0].set_title('Model Comparison - Test Set Performance')\n",
                "axes[0].set_xticks(x)\n",
                "axes[0].set_xticklabels(metrics_short, rotation=45)\n",
                "axes[0].legend()\n",
                "axes[0].set_ylim([0, 1.1])\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Performance across splits (AUC)\n",
                "splits = ['Train', 'Validation', 'Test']\n",
                "glm_aucs = [glm_results['train_auc'], glm_results['validation_auc'], glm_results['test_auc']]\n",
                "xgb_aucs = [xgb_results['train_auc'], xgb_results['validation_auc'], xgb_results['test_auc']]\n",
                "\n",
                "axes[1].plot(splits, glm_aucs, marker='o', label='GLM', linewidth=2, markersize=8, color='skyblue')\n",
                "axes[1].plot(splits, xgb_aucs, marker='s', label='XGBoost', linewidth=2, markersize=8, color='coral')\n",
                "axes[1].set_xlabel('Data Split')\n",
                "axes[1].set_ylabel('AUC Score')\n",
                "axes[1].set_title('AUC Performance Across Data Splits')\n",
                "axes[1].legend()\n",
                "axes[1].set_ylim([0, 1.1])\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(output_dir / 'plots' / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save comparison results\n",
                "results_dir = output_dir / 'results'\n",
                "comparison_df.to_csv(results_dir / 'model_comparison.csv', index=False)\n",
                "\n",
                "# Save detailed results\n",
                "with open(results_dir / 'detailed_results.pkl', 'wb') as f:\n",
                "    pickle.dump(results, f)\n",
                "\n",
                "print(f\"‚úì Comparison results saved to: {results_dir}\")\n",
                "print(\"‚úì Phase 5 (Model Comparison) completed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Interpretability (LIME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"MODEL INTERPRETABILITY ANALYSIS (LIME)\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "if not LIME_AVAILABLE:\n",
                "    print(\"\\n‚ö† LIME not available. Skipping interpretability analysis.\")\n",
                "    print(\"Install with: pip install lime\")\n",
                "else:\n",
                "    print(\"\\n‚úì LIME is available\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if LIME_AVAILABLE:\n",
                "    # Create LIME explainer\n",
                "    print(\"\\nCreating LIME explainer...\")\n",
                "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
                "        X_train.values,\n",
                "        feature_names=X_train.columns.tolist(),\n",
                "        class_names=['Class 0', 'Class 1'],\n",
                "        mode='classification'\n",
                "    )\n",
                "    print(\"‚úì LIME explainer created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if LIME_AVAILABLE:\n",
                "    # Generate explanations for sample instances\n",
                "    print(\"\\nGenerating LIME explanations for sample test instances...\")\n",
                "    sample_indices = np.random.choice(len(X_test), size=min(10, len(X_test)), replace=False)\n",
                "    \n",
                "    explanations = {}\n",
                "    \n",
                "    for model_name, model in [('GLM', glm_model), ('XGBoost', xgb_model)]:\n",
                "        print(f\"\\nGenerating explanations for {model_name}...\")\n",
                "        explanations[model_name] = []\n",
                "        \n",
                "        for i, idx in enumerate(sample_indices):\n",
                "            instance = X_test.iloc[idx].values\n",
                "            \n",
                "            # Generate explanation\n",
                "            exp = explainer.explain_instance(\n",
                "                instance,\n",
                "                model.predict_proba,\n",
                "                num_features=10\n",
                "            )\n",
                "            \n",
                "            explanations[model_name].append({\n",
                "                'instance_id': idx,\n",
                "                'prediction': model.predict_proba([instance])[0],\n",
                "                'explanation': exp.as_list()\n",
                "            })\n",
                "        \n",
                "        print(f\"  ‚úì Generated {len(sample_indices)} explanations for {model_name}\")\n",
                "    \n",
                "    # Save explanations\n",
                "    results_dir = output_dir / 'results'\n",
                "    with open(results_dir / 'lime_explanations.pkl', 'wb') as f:\n",
                "        pickle.dump(explanations, f)\n",
                "    \n",
                "    print(f\"\\n‚úì LIME explanations saved to: {results_dir / 'lime_explanations.pkl'}\")\n",
                "    print(\"‚úì Phase 6 (Interpretability) completed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if LIME_AVAILABLE:\n",
                "    # Display sample explanation\n",
                "    print(\"\\nSample LIME Explanation (XGBoost, Instance 0):\")\n",
                "    print(\"=\" * 50)\n",
                "    sample_exp = explanations['XGBoost'][0]\n",
                "    print(f\"Prediction probabilities: {sample_exp['prediction']}\")\n",
                "    print(f\"\\nTop contributing features:\")\n",
                "    for feat, contrib in sample_exp['explanation'][:10]:\n",
                "        print(f\"  {feat}: {contrib:+.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Final Documentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"FINAL DOCUMENTATION\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate final report\n",
                "report_path = output_dir / 'final_report.txt'\n",
                "\n",
                "with open(report_path, 'w') as f:\n",
                "    f.write(\"=\" * 80 + \"\\n\")\n",
                "    f.write(\"GLM vs XGBoost Pipeline - Final Report\\n\")\n",
                "    f.write(\"=\" * 80 + \"\\n\")\n",
                "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
                "    \n",
                "    # Dataset summary\n",
                "    f.write(\"DATASET SUMMARY:\\n\")\n",
                "    f.write(f\"- Original shape: {df_original.shape}\\n\")\n",
                "    f.write(f\"- Engineered shape: {df_engineered.shape}\\n\")\n",
                "    f.write(f\"- Training samples: {len(X_train)}\\n\")\n",
                "    f.write(f\"- Validation samples: {len(X_val)}\\n\")\n",
                "    f.write(f\"- Test samples: {len(X_test)}\\n\\n\")\n",
                "    \n",
                "    # Model results\n",
                "    f.write(\"MODEL PERFORMANCE:\\n\")\n",
                "    for model_name, res in results.items():\n",
                "        f.write(f\"\\n{res['model_name']}:\\n\")\n",
                "        f.write(f\"  Test AUC:      {res.get('test_auc', 'N/A'):.4f}\\n\")\n",
                "        f.write(f\"  Test Accuracy: {res.get('test_accuracy', 'N/A'):.4f}\\n\")\n",
                "        f.write(f\"  Test F1-Score: {res.get('test_f1', 'N/A'):.4f}\\n\")\n",
                "        f.write(f\"  Test Recall:   {res.get('test_recall', 'N/A'):.4f}\\n\")\n",
                "        f.write(f\"  Test Precision:{res.get('test_precision', 'N/A'):.4f}\\n\")\n",
                "    \n",
                "    f.write(f\"\\n{'='*50}\\n\")\n",
                "    f.write(f\"BEST MODEL: {best_model_name.upper()} (Test AUC: {best_auc:.4f})\\n\")\n",
                "    f.write(f\"{'='*50}\\n\")\n",
                "    \n",
                "    f.write(f\"\\nOutput directory: {output_dir}\\n\")\n",
                "    f.write(\"Pipeline completed successfully!\\n\")\n",
                "\n",
                "print(f\"‚úì Final report saved to: {report_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display final summary\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"PIPELINE SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"\\nüìä Dataset:\")\n",
                "print(f\"   - Original shape: {df_original.shape}\")\n",
                "print(f\"   - Engineered features: {X_train.shape[1]}\")\n",
                "print(f\"   - Train/Val/Test: {len(X_train)}/{len(X_val)}/{len(X_test)} samples\")\n",
                "\n",
                "print(f\"\\nü§ñ Models Trained:\")\n",
                "print(f\"   - GLM (Logistic Regression)\")\n",
                "print(f\"   - XGBoost\")\n",
                "\n",
                "print(f\"\\nüèÜ Best Model: {best_model_name.upper()}\")\n",
                "print(f\"   - Test AUC: {best_auc:.4f}\")\n",
                "\n",
                "print(f\"\\nüìÅ Output Directory: {output_dir}\")\n",
                "print(f\"   - Models saved in: {output_dir / 'models'}\")\n",
                "print(f\"   - Results saved in: {output_dir / 'results'}\")\n",
                "print(f\"   - Plots saved in: {output_dir / 'plots'}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"‚úÖ COMPLETE PIPELINE FINISHED SUCCESSFULLY!\")\n",
                "print(\"=\" * 80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
